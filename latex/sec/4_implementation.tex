% \section{Implementation}
% this section will talk about how we trained our models 

% \subsection{Preprocessing}
% something about data augmentation 

% \subsection{Training}
% something about VGG that we did some result 


% \subsection{placeholder for table}

% compare both models on training metrics

% \subsection{Evaluation}
% something about VGG that we did some result 


% \subsection{placeholder for table}

% compare both models on acc and epochs it took to train 




% \subsection{Hyperparameters}
% Both VGG16 and ResNet models were trained using transfer learning with fine-tuning of the upper convolutional layers and full retraining of the fully connected layers. We used the Adam optimizer with a learning rate scheduler for efficient convergence. VGG16 demonstrated faster initial convergence, while ResNet maintained better generalization on unseen validation data. The models were trained for multiple epochs until the validation loss plateaued.
% \begin{table}[ht]
% \centering
% \caption{Crop disease status}
% \label{tab:crop-disease-status}
% \begin{tabular}{lc}
% \toprule
% Hyperparameter & Parameter Value  \\

% \midrule
% Optimizer & --- \\
% Loss & ---  \\
% Learning Rate & ---  \\
% Batch Size & ---  \\
% Epochs & ---  \\
% Activation Function & ---  \\

% \bottomrule
% \end{tabular}
% \end{table}



% \begin{table}[ht]
% \centering
% \caption{Crop disease status}
% \label{tab:crop-disease-status}
% \begin{tabular}{lccc}
% \toprule
% Model & Training Accuracy & Validation Loss & Time per Epoch \\

% \midrule
% VGG16 & --- & --- & ---\\
% ResNet & --- & --- & --- \\

% \bottomrule
% \end{tabular}
% \end{table}


\section{Methodology for Comparative Evaluation}

Deep learning architectures used for classification are often pretrained on large-scale datasets such as ImageNet, which contain millions of labeled images across diverse categories. Leveraging these pretrained models provides a strong initialization for feature extraction, thereby reducing the need for extensive training from scratch\cite{gupta2022deep}. This approach not only saves computational resources but also accelerates convergence during fine-tuning. In this study, transfer learning is employed to adapt these pretrained convolutional backbones for the classification of plant leaf health conditions.

During the fine-tuning process, the early layers of the network responsible for learning general low-level features such as edges, textures, and color gradients are typically frozen to preserve their pretrained weights. The final classification layers, however, are unfrozen and retrained using the PlantVillage dataset to learn domain-specific patterns related to diseased and healthy leaves. This selective retraining ensures that the network retains its general visual understanding while adapting effectively to the specific task of plant disease recognition. Fine-tuning was performed for a limited number of epochs to prevent overfitting, given the relatively smaller dataset size compared to ImageNet.

\subsection{Data Augmentation}
Data augmentation increases the effective size and diversity of the training set by applying label-preserving transformations. For minority classes this helps the model see more varied examples of disease symptoms, reducing the risk that the classifier will ignore rare categories. For majority classes we perform selective augmentation and sometimes replace original images with augmented variants for two reasons:

First, prevent overfitting to trivial background cues: PlantVillage images were collected in controlled conditions with uniform backgrounds. If the model memorizes background or camera-specific artifacts present in the majority class, it may not generalize to field images. Replacing some majority-class originals with augmented variants (cropped, color-jittered, or slightly rotated) encourages the model to learn disease-relevant features instead of spurious patterns.

Second, Maintain class balance in batches while preserving dataset size: Instead of only increasing the dataset by duplicating/augmenting majority classes (which worsens imbalance), we use augmentation to create alternative views and then sample batches that are balanced across the two binary labels (healthy vs unhealthy). This preserves the overall dataset scale while providing more diverse training signals.

We used the PlantVillage dataset (loaded via TensorFlow Datasets) and converted the original multi-class labels of the form ``PlantType\_\_\_DiseaseName'' into a binary target by assigning $0 = \text{healthy}$ and $1 = \text{diseased}$ (determined by testing whether the suffix after ``\_\_\_'' equals ``healthy''). 

To mitigate class imbalance we applied a class-specific augmentation and replication scheme implemented in a tf.data pipeline: labels are mapped to binary via a static lookup array and all image augmentations are performed on float32 images in the $\{0,1\}$ range and converted back to uint8 for downstream use. For the healthy class we applied aggressive stochastic augmentations: random horizontal and vertical flips, a random $90^\circ$ rotation $(k \in {0,1,2,3})$, random saturation in $[0.8,1.25]$, random hue offset up to $\pm0.05$, random brightness delta up to $\pm0.12$, and random contrast in $[0.8,1.25]$. For the diseased class we applied milder perturbations and a replacement policy: for each diseased sample, with probability 0.5 we replace the original with an augmented variant produced by random horizontal flip, random $90^\circ$ rotation, contrast in $[0.9,1.1]$, brightness $\pm0.08$ and hue $\pm0.03$; otherwise the original image is retained. (See Figure~\ref{fig:Aug} for example augmentations.)

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{sec/Images/augumentation_example.png}
    \caption{Example of Data-Augmentation}
    \label{fig:Aug}
\end{figure}

To balance classes we computed H and D as the healthy and diseased training counts and set the healthy replication multiplier $m = \max(1, \ceil{(D/H)} - 1)\quad(\text{if }H>0)$, then concatenated the original healthy set with m independently-augmented passes of the healthy set to produce $\approx H\times (m+1)$ healthy examples; augmented diseased samples were concatenated thereafter. See Table~\ref{fig:dist_Aug} for the final class distribution after augmentation.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{sec/Images/healthy_vs_diseased_aug.png}
%     \caption{Distribution after augmentation}
%     \label{fig:dist_Aug}
% \end{figure}

\begin{table}[ht]
\centering
\caption{Distribution after augmentation}
\label{fig:dist_Aug}
\begin{tabular}{lc}
\toprule
Class & Samples \\
\midrule
Healthy & 45249 (53\%) \\
Diseased & 39220 (47\%) \\
\midrule
\textbf{Total} & \textbf{84469} \\
\bottomrule
\end{tabular}
\end{table}

The combined training set is shuffled (buffer sizes used: 4096 for healthy-stage shuffling, 8192 for final shuffling), prefetched with tf.data.AUTOTUNE, and used for training. For reproducibility we logged TensorFlow and tfds versions, dataset split, and random seeds (tf.random.set\_seed plus numpy/python RNGs) and stored the exact augmentation hyperparameters (saturation, hue, brightness, contrast ranges and diseased replacement probability) alongside experiment metadata.

\subsection{Evaluation Metrics}

To ensure a fair and comprehensive comparison across different pretrained architectures, several quantitative metrics were employed to evaluate model performance. The primary metric used was \textbf{accuracy}, defined as the ratio of correctly classified samples to the total number of samples. While accuracy provides a broad view of performance, it may not fully capture the nuances of class imbalance within the dataset.

Therefore, additional performance metrics were calculated, including \textbf{precision}, \textbf{recall}, and the \textbf{F1-score}. Precision measures the proportion of correctly predicted positive samples out of all samples predicted as positive, whereas recall measures the proportion of correctly predicted positive samples relative to all actual positive samples. The F1-score, being the harmonic mean of precision and recall, offers a balanced assessment of a model's ability to handle false positives and false negatives. These metrics collectively provide a more robust understanding of model behavior under different conditions\cite{oak2024comparative}.

Furthermore, model performance was also analyzed using the \textbf{Receiver Operating Characteristic (ROC)} curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds. The \textbf{Area Under the Curve (AUC)} serves as a single scalar value summarizing the overall discriminative capability of the modelâ€”where a higher AUC indicates stronger classification performance.

\textbf{Confusion matrices} were generated for each model to provide a detailed breakdown of classification outcomes, including True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). From the confusion matrix, we also derived \textbf{sensitivity} (also known as recall or true positive rate), which measures the model's ability to correctly identify diseased samples, and \textbf{specificity} (true negative rate), which measures the model's ability to correctly identify healthy samples. These metrics are particularly important in agricultural applications where both false alarms and missed detections carry significant consequences.

Additionally, \textbf{inference speed} (samples per second) was measured to assess the practical deployability of each model, as real-world plant disease detection systems often require rapid classification for timely intervention. Such a multi-metric evaluation ensures that both overall accuracy, per-class discrimination, and computational efficiency are considered in determining the most effective pretrained model for plant disease detection.

\subsection{Cross-Dataset Evaluation on PlantDoc}

To assess the generalization capability of the trained models beyond the PlantVillage dataset, we conducted cross-dataset evaluation using the PlantDoc dataset~\cite{plantdoc2020}. This evaluation is critical because models trained on PlantVillage's controlled laboratory images may overfit to dataset-specific artifacts such as uniform backgrounds and consistent lighting conditions~\cite{noyan2022uncovering}. Testing on PlantDoc, which contains internet-scraped images with natural backgrounds and varying conditions, provides a realistic assessment of model robustness.

The cross-dataset evaluation pipeline was implemented using PyTorch with the following methodology:

\textbf{Preprocessing:} All test images from PlantDoc were resized to $224 \times 224$ pixels to match the input dimensions used during training. Standard ImageNet normalization (mean $= [0.485, 0.456, 0.406]$, std $= [0.229, 0.224, 0.225]$) was applied to ensure consistency with the pretrained model expectations.

\textbf{Label Mapping:} The PlantDoc dataset's multi-class folder structure was converted to binary labels. A rule-based classifier examined folder names for disease-indicating keywords (e.g., ``rust'', ``blight'', ``spot'', ``scab'', ``mildew'', ``bacterial'', ``virus''). Folders without these keywords and containing only ``$<$PlantType$>$ leaf'' patterns were classified as healthy. This automated mapping achieved consistent labeling across all 27 original categories.

\textbf{Inference:} Models were loaded from their trained checkpoints and evaluated in inference mode with mixed-precision (FP16) for computational efficiency. For each test image, the model produced class probabilities via softmax, and the predicted class was determined by argmax. The probability of the diseased class was retained for ROC analysis.

\textbf{Metrics Computation:} The same comprehensive metrics used for PlantVillage evaluation were computed on the PlantDoc test set: accuracy, precision, recall, F1-score, and AUC-ROC. Confusion matrices were generated to analyze per-class performance, with particular attention to sensitivity (true positive rate for diseased samples) and specificity (true negative rate for healthy samples). This cross-dataset evaluation reveals how well each architecture captures generalizable disease features versus dataset-specific patterns.