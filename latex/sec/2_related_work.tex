% FIX: Instead of talking about various architectures in isolation, frame this section around how this work differs from existing literature in scope and validation methodology.
\section{Related Work}

Comparative analyses of CNN models are crucial for selecting appropriate architectures for specific applications. Existing surveys often provide comprehensive overviews of architecture, applications, and performance on standard datasets like ImageNet, MNIST, and CIFAR-10/100. These studies systematically evaluate metrics like accuracy, parameter count, and FLOPs to provide insights into model suitability.

CNNs have found wide application across various domains, including healthcare, remote sensing, security, and agriculture. In agriculture, convolutional neural networks are specifically pointed out as being useful for tasks like monitoring crops, spotting pests, predicting harvest amounts, and detecting/treating diseases. Given the complexity of visual data in agricultural settings, deep learning is essential for automated feature extraction, moving beyond traditional manual feature engineering methods.

The general strategy employed in this domain is \textbf{transfer learning}. Because acquiring large, richly annotated agricultural datasets is often resource-intensive and time-consuming, researchers leverage models pretrained on massive general visual datasets (like ImageNet) and fine-tune them for specific PDD tasks. Reviews specifically addressing CNN use in agriculture emphasize the utility of these techniques for maximizing resource use and accuracy. \cite{MajeedZangana2024,trigka2025comprehensive, li2021survey,chen2021review}

\subsection{Differentiation of the Current Research}
This research, differs significantly from existing literature in both scope and rigorous validation methodology, specifically addressing the practical challenges of deployment in agricultural informatics:
\begin{enumerate}
    \item \textbf{Comprehensive Architectural Scope:} We systematically evaluate and compare a broad spectrum of pretrained CNN models, ranging from established deep architectures (e.g., VGG16, ResNet50, DenseNet-121, Inception V4, Xception) to contemporary efficient and lightweight designs (e.g., MobileNet, ShuffleNet, MnasNet1, EfficientNet-Lite4, ConvNext). This breadth allows for identifying not only the most accurate models but also those offering the best balance between accuracy and computational efficiency (measured via inference speed), which is paramount for practical edge deployment.
    \item \textbf{Reframing the Classification Task:} Instead of focusing purely on multi-class disease differentiation, this study recasts the problem into a crucial \textbf{binary classification} task: distinguishing "healthy" from "unhealthy" leaves. This shift, though seemingly simpler, demands sophisticated techniques to mitigate the aggravated class imbalance and the risk of overfitting to spurious cues. We employ targeted data augmentation (including replacing majority class images with augmented variants to encourage learning disease-relevant features rather than background artifacts) and batch-wise balanced sampling to ensure robust training.
    \item \textbf{Cross-Dataset Generalization Validation:} Crucially, we move beyond reporting performance solely on the clean, in-distribution PlantVillage test set. We deploy a \textbf{cross-dataset evaluation} strategy using the independent PlantDoc dataset. The PlantDoc dataset, derived from internet-scraped images, inherently possesses the real-world domain variations (natural backgrounds, diverse lighting) that models trained on PlantVillage typically fail to generalize to. This robust evaluation, measured using comprehensive metrics (including precision, recall, F1-score, AUC-ROC, sensitivity, and specificity), provides a realistic assessment of model deployability and robustness to environmental shiftsâ€”a critical but often overlooked metric in prior PDD studies.
\end{enumerate}