{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7edfba2",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch Training\n",
    "Train frozen-backbone torch models on the same preprocessed PlantVillage dataset (no augmentation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95e1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "GPU/Accelerator: Apple MPS\n",
      "\n",
      "================================================================================\n",
      "Training: mobilenet_v2\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training: mobilenet_v2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v2] Epoch 1 | Train Loss: 0.2882 Acc: 0.9549 | Val Loss: 0.1786 Acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v2] Epoch 2 | Train Loss: 0.1683 Acc: 0.9628 | Val Loss: 0.1408 Acc: 0.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v2] Epoch 3 | Train Loss: 0.1434 Acc: 0.9627 | Val Loss: 0.1387 Acc: 0.9477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v2] Epoch 4 | Train Loss: 0.1347 Acc: 0.9666 | Val Loss: 0.1246 Acc: 0.9517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v2] Epoch 5 | Train Loss: 0.1293 Acc: 0.9690 | Val Loss: 0.1167 Acc: 0.9575\n",
      "\n",
      "================================================================================\n",
      "Training: shufflenet_v2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2] Epoch 1 | Train Loss: 0.6165 Acc: 0.9091 | Val Loss: 0.5665 Acc: 0.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2] Epoch 2 | Train Loss: 0.5012 Acc: 0.9246 | Val Loss: 0.4661 Acc: 0.9024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2] Epoch 3 | Train Loss: 0.4243 Acc: 0.9294 | Val Loss: 0.4092 Acc: 0.9052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2] Epoch 4 | Train Loss: 0.3703 Acc: 0.9341 | Val Loss: 0.3587 Acc: 0.9138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2] Epoch 5 | Train Loss: 0.3321 Acc: 0.9407 | Val Loss: 0.3107 Acc: 0.9289\n",
      "\n",
      "================================================================================\n",
      "Training: squeezenet1_0\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[squeezenet1_0] Epoch 1 | Train Loss: 0.6218 Acc: 0.5928 | Val Loss: 0.4272 Acc: 0.7825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[squeezenet1_0] Epoch 2 | Train Loss: 0.5326 Acc: 0.6364 | Val Loss: 0.4002 Acc: 0.8022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[squeezenet1_0] Epoch 3 | Train Loss: 0.5195 Acc: 0.6551 | Val Loss: 0.3846 Acc: 0.8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[squeezenet1_0] Epoch 4 | Train Loss: 0.5132 Acc: 0.6954 | Val Loss: 0.3861 Acc: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[squeezenet1_0] Epoch 5 | Train Loss: 0.5091 Acc: 0.7225 | Val Loss: 0.3859 Acc: 0.8516\n",
      "Early stopping triggered.\n",
      "\n",
      "================================================================================\n",
      "Training: mnasnet1_0\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mnasnet1_0] Epoch 1 | Train Loss: 0.3136 Acc: 0.6666 | Val Loss: 1.1109 Acc: 0.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mnasnet1_0] Epoch 2 | Train Loss: 0.1838 Acc: 0.7386 | Val Loss: 1.0217 Acc: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mnasnet1_0] Epoch 3 | Train Loss: 0.1596 Acc: 0.7796 | Val Loss: 0.8668 Acc: 0.6683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mnasnet1_0] Epoch 4 | Train Loss: 0.1449 Acc: 0.8324 | Val Loss: 0.6349 Acc: 0.7431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mnasnet1_0] Epoch 5 | Train Loss: 0.1368 Acc: 0.8951 | Val Loss: 0.3796 Acc: 0.8446\n",
      "\n",
      "================================================================================\n",
      "Training: tf_efficientnet_lite4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf_efficientnet_lite4] Epoch 1 | Train Loss: 0.2612 Acc: 0.9656 | Val Loss: 0.1124 Acc: 0.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf_efficientnet_lite4] Epoch 2 | Train Loss: 0.1072 Acc: 0.9756 | Val Loss: 0.0990 Acc: 0.9685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf_efficientnet_lite4] Epoch 3 | Train Loss: 0.0774 Acc: 0.9725 | Val Loss: 0.0861 Acc: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf_efficientnet_lite4] Epoch 4 | Train Loss: 0.0617 Acc: 0.9854 | Val Loss: 0.0793 Acc: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf_efficientnet_lite4] Epoch 5 | Train Loss: 0.0554 Acc: 0.9896 | Val Loss: 0.0527 Acc: 0.9836\n",
      "\n",
      "================================================================================\n",
      "Saved complete training history for all models to: results/all_models_training_history.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from tqdm import tqdm   # NEW\n",
    "\n",
    "# ======================================================\n",
    "# CONFIG\n",
    "# ======================================================\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "PATIENCE = 2\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "NPY_DIR = ROOT / \"preprocessed_numpy\"\n",
    "RESULTS_DIR = ROOT / \"results\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# DEVICE \n",
    "# ======================================================\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**2)\n",
    "    gpu_used = torch.cuda.memory_allocated() / (1024**2)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    gpu_name = \"Apple MPS\"\n",
    "    gpu_total = 0\n",
    "    gpu_used = 0\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    gpu_name = \"CPU\"\n",
    "    gpu_total = 0\n",
    "    gpu_used = 0\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"GPU/Accelerator:\", gpu_name)\n",
    "\n",
    "# ======================================================\n",
    "# DATASET\n",
    "# ======================================================\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.images = np.load(NPY_DIR / f\"{split}_images.npy\")\n",
    "        self.labels = np.load(NPY_DIR / f\"{split}_labels.npy\")\n",
    "        self.transform = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.images[idx]), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def make_loader(split):\n",
    "    return DataLoader(\n",
    "        NumpyDataset(split),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=(split == \"train\"),\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "train_loader = make_loader(\"train\")\n",
    "val_loader = make_loader(\"val\")\n",
    "test_loader = make_loader(\"test\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# MODEL BUILDER\n",
    "# ======================================================\n",
    "def build_model(name, freeze_backbone=True):\n",
    "    lname = name.lower()\n",
    "\n",
    "    if lname == \"resnet50\":\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "    elif lname == \"alexnet\":\n",
    "        model = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "        model.classifier[6] = nn.Linear(4096, 1)\n",
    "\n",
    "    elif lname == \"vgg16\":\n",
    "        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        model.classifier[6] = nn.Linear(4096, 1)\n",
    "\n",
    "    elif lname == \"densenet121\":\n",
    "        model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "\n",
    "    elif lname == \"googlenet\":\n",
    "        model = models.googlenet(\n",
    "            weights=models.GoogLeNet_Weights.IMAGENET1K_V1,\n",
    "        )\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "        # Set aux_logits to False after initialization\n",
    "        model.aux_logits = False\n",
    "\n",
    "    elif lname == \"shufflenet_v2\":\n",
    "        model = models.shufflenet_v2_x1_0(weights=models.ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "    elif lname == \"mobilenet_v2\":\n",
    "        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "    elif lname == \"squeezenet1_0\":\n",
    "        model = models.squeezenet1_0(weights=models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        # Replace the final conv layer (classifier)\n",
    "        model.classifier[1] = nn.Conv2d(512, 1, kernel_size=1)\n",
    "        model.num_classes = 1\n",
    "\n",
    "    elif lname == \"mnasnet1_0\":\n",
    "        model = models.mnasnet1_0(weights=models.MNASNet1_0_Weights.IMAGENET1K_V1)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "\n",
    "\n",
    "    else:\n",
    "        import timm\n",
    "        model = timm.create_model(lname, pretrained=True, num_classes=1)\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for n, p in model.named_parameters():\n",
    "            if \"fc\" not in n and \"classifier\" not in n and \"head\" not in n:\n",
    "                p.requires_grad = False\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# METRICS\n",
    "# ======================================================\n",
    "def eval_metrics(model, loader, criterion=None, return_raw=False):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    total_loss, n = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            if criterion:\n",
    "                total_loss += criterion(logits, y).item() * X.size(0)\n",
    "\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_prob.extend(probs.cpu().numpy())\n",
    "            n += X.size(0)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    try: auc = roc_auc_score(y_true, y_prob)\n",
    "    except: auc = float(\"nan\")\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "\n",
    "    if return_raw:\n",
    "        return y_true, y_prob, fpr, tpr, thr\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n if criterion else None,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"auc\": auc,\n",
    "        \"roc_fpr\": fpr,\n",
    "        \"roc_tpr\": tpr,\n",
    "        \"roc_thresh\": thr,\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# TRAINING LOOP WITH tqdm\n",
    "# ======================================================\n",
    "def train_models(models_to_train):\n",
    "    # Load existing history if JSON file exists\n",
    "    json_path = RESULTS_DIR / \"all_models_training_history.json\"\n",
    "    if json_path.exists():\n",
    "        with open(json_path, 'r') as f:\n",
    "            all_models_history = json.load(f)\n",
    "    else:\n",
    "        all_models_history = {}\n",
    "\n",
    "    for name in models_to_train:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Training: {name}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        model = build_model(name)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=1)\n",
    "\n",
    "        best_loss = float(\"inf\")\n",
    "        best_epoch = 0\n",
    "        best_path = MODELS_DIR / f\"{name}_best.pth\"\n",
    "        no_improve = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        # Track per-epoch metrics\n",
    "        epoch_history = []\n",
    "\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            \n",
    "\n",
    "            # tqdm progress bar for batches\n",
    "            batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} ({name})\", leave=False)\n",
    "\n",
    "            model.train()\n",
    "            total_loss, n = 0.0, 0\n",
    "\n",
    "            for X, y in batch_bar:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(X).squeeze(1)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * X.size(0)\n",
    "                n += X.size(0)\n",
    "                batch_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "            train_loss = total_loss / n\n",
    "            \n",
    "            # Calculate train accuracy\n",
    "            train_m = eval_metrics(model, train_loader, criterion)\n",
    "            train_acc = train_m[\"accuracy\"]\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_m = eval_metrics(model, val_loader, criterion)\n",
    "            val_loss = val_m[\"loss\"]\n",
    "            val_acc = val_m[\"accuracy\"]\n",
    "            \n",
    "            # Calculate confusion matrix for validation\n",
    "            val_y_true, val_y_prob, _, _, _ = eval_metrics(model, val_loader, criterion, return_raw=True)\n",
    "            val_y_pred = (val_y_prob > 0.5).astype(int)\n",
    "            val_cm = confusion_matrix(val_y_true, val_y_pred)\n",
    "            val_tn, val_fp, val_fn, val_tp = val_cm.ravel()\n",
    "            \n",
    "            # Save epoch metrics\n",
    "            epoch_history.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"val_tn\": int(val_tn),\n",
    "                \"val_fp\": int(val_fp),\n",
    "                \"val_fn\": int(val_fn),\n",
    "                \"val_tp\": int(val_tp)\n",
    "            })\n",
    "\n",
    "            print(f\"[{name}] Epoch {epoch} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), best_path)\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # Final evaluation\n",
    "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "        val_m = eval_metrics(model, val_loader, criterion)\n",
    "        test_m = eval_metrics(model, test_loader, criterion)\n",
    "        \n",
    "        # Get confusion matrices for final metrics\n",
    "        val_y_true, val_y_prob, _, _, _ = eval_metrics(model, val_loader, criterion, return_raw=True)\n",
    "        val_y_pred = (val_y_prob > 0.5).astype(int)\n",
    "        val_cm = confusion_matrix(val_y_true, val_y_pred)\n",
    "        val_tn, val_fp, val_fn, val_tp = val_cm.ravel()\n",
    "        \n",
    "        test_y_true, test_y_prob, fpr, tpr, thr = eval_metrics(model, test_loader, criterion, return_raw=True)\n",
    "        test_y_pred = (test_y_prob > 0.5).astype(int)\n",
    "        test_cm = confusion_matrix(test_y_true, test_y_pred)\n",
    "        test_tn, test_fp, test_fn, test_tp = test_cm.ravel()\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        # Store this model's training history\n",
    "        all_models_history[name] = {\n",
    "            \"model\": name,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"total_epochs_trained\": len(epoch_history),\n",
    "            \"epochs\": epoch_history,\n",
    "            \"final_metrics\": {\n",
    "                \"val_loss\": val_m[\"loss\"],\n",
    "                \"val_accuracy\": val_m[\"accuracy\"],\n",
    "                \"val_precision\": val_m[\"precision\"],\n",
    "                \"val_recall\": val_m[\"recall\"],\n",
    "                \"val_f1\": val_m[\"f1\"],\n",
    "                \"val_auc\": val_m[\"auc\"],\n",
    "                \"val_confusion_matrix\": {\"tn\": int(val_tn), \"fp\": int(val_fp), \"fn\": int(val_fn), \"tp\": int(val_tp)},\n",
    "                \"test_loss\": test_m[\"loss\"],\n",
    "                \"test_accuracy\": test_m[\"accuracy\"],\n",
    "                \"test_precision\": test_m[\"precision\"],\n",
    "                \"test_recall\": test_m[\"recall\"],\n",
    "                \"test_f1\": test_m[\"f1\"],\n",
    "                \"test_auc\": test_m[\"auc\"],\n",
    "                \"test_confusion_matrix\": {\"tn\": int(test_tn), \"fp\": int(test_fp), \"fn\": int(test_fn), \"tp\": int(test_tp)},\n",
    "            },\n",
    "            \"training_info\": {\n",
    "                \"train_time_sec\": elapsed\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Save all models' training history to one JSON file\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(all_models_history, f, indent=2)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Saved complete training history for all models to: {json_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "# ======================================================\n",
    "# RUN\n",
    "# ======================================================\n",
    "models_to_train = [\n",
    "\n",
    "    # ============================\n",
    "    # Lightweight Architectures\n",
    "    # ============================\n",
    "\n",
    "    # \"mobilenet_v2\",           # Lightweight CNN | Pratyaksh done\n",
    "    # \"shufflenet_v2\",          # Lightweight CNN | Pratyaksh done\n",
    "    # \"squeezenet1_0\",          # Lightweight CNN (Fire modules) | Pratyaksh done\n",
    "    # \"mnasnet1_0\",             # Lightweight CNN (NAS-designed) | Pratyaksh done\n",
    "    # \"tf_efficientnet_lite4\",  # Lightweight CNN (EfficientNet-Lite) | Pratyaksh done\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # Classic CNNs\n",
    "    # ============================\n",
    "    \"googlenet\",              # Classic CNN (Inception V1) | Pratyaksh  done\n",
    "    \"alexnet\",                # Classic CNN  | pratyaksh done\n",
    "    \"vgg16\",                  # Classic CNN  | Prartyaksh done\n",
    "    \"resnet50\",               # Classic CNN  | Pratyaksh done\n",
    "    \"densenet121\",            # Classic CNN (Dense connections) | Pratyaksh done\n",
    "\n",
    "    # \"convnext_base\",          # Deep CNN (ConvNeXt â€” CNN inspired by ViT) | Vandit\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "train_models(models_to_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db182463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
