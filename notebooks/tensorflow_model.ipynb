{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2324a3b8",
   "metadata": {},
   "source": [
    "\n",
    "# TensorFlow Training\n",
    "Train multiple frozen-backbone classifiers on the preprocessed PlantVillage dataset. No augmentation here; dataset is already balanced/augmented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69cd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "PREPROCESSED_DIR = PROJECT_ROOT / \"preprocessed\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be50d5",
   "metadata": {},
   "source": [
    "## 1) Data loaders (PNG + CSV labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce27e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_split(split_name):\n",
    "    csv_path = PREPROCESSED_DIR / f\"{split_name}_labels.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    filepaths = str(PREPROCESSED_DIR / split_name) + os.sep + df[\"filename\"].astype(str)\n",
    "    labels = df[\"label\"].astype(np.int32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "\n",
    "    def _load(path, label):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.io.decode_png(image, channels=3)\n",
    "        image = tf.image.resize(image, IMAGE_SIZE)\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image, tf.cast(label, tf.float32)\n",
    "\n",
    "    return ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = load_split(\"train\")\n",
    "val_ds = load_split(\"val\")\n",
    "test_ds = load_split(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b19cb5",
   "metadata": {},
   "source": [
    "## 2) Model factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03832976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_binary_classifier(base_model_fn, input_shape=(224, 224, 3)):\n",
    "    base_model = base_model_fn(input_shape=input_shape, include_top=False, weights=\"imagenet\")\n",
    "    base_model.trainable = False\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_builder(model_key):\n",
    "    key = model_key.lower()\n",
    "    if key == \"mobilenet_v2\":\n",
    "        return lambda: create_binary_classifier(tf.keras.applications.MobileNetV2)\n",
    "    if key == \"efficientnet_b0\":\n",
    "        return lambda: create_binary_classifier(tf.keras.applications.EfficientNetB0)\n",
    "    if key == \"resnet50\":\n",
    "        return lambda: create_binary_classifier(tf.keras.applications.ResNet50)\n",
    "    if key == \"vgg16\":\n",
    "        return lambda: create_binary_classifier(tf.keras.applications.VGG16)\n",
    "    if key == \"densenet121\":\n",
    "        return lambda: create_binary_classifier(tf.keras.applications.DenseNet121)\n",
    "    if key == \"inception_v3\":\n",
    "        return lambda: create_binary_classifier(tf.keras.applications.InceptionV3)\n",
    "    raise ValueError(f\"Unknown model key: {model_key}\")\n",
    "\n",
    "\n",
    "MODELS_TO_TRAIN = [\n",
    "#    \"mobilenet_v2\", \n",
    "#    \"efficientnet_b0\",\n",
    "    \"resnet50\",\n",
    "                   ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eeef7c",
   "metadata": {},
   "source": [
    "## 3) Training + evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed84ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Training resnet50 ===\n",
      "Epoch 1/5\n",
      "\u001b[1m1510/2186\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 154ms/step - accuracy: 0.9602 - auc: 0.2584 - loss: 0.0783 - precision: 0.1095 - recall: 0.1000"
     ]
    }
   ],
   "source": [
    "\n",
    "def compile_model(model, lr=1e-3):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_metrics(model, test_ds):\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    for images, labels in test_ds:\n",
    "        probs = model.predict(images, verbose=0).flatten()\n",
    "        y_prob.extend(probs)\n",
    "        y_true.extend(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred)),\n",
    "        \"auc\": float(roc_auc_score(y_true, y_prob)),\n",
    "    }\n",
    "\n",
    "\n",
    "all_rows = []\n",
    "for model_key in MODELS_TO_TRAIN:\n",
    "    print(f\" === Training {model_key} ===\")\n",
    "    builder = get_model_builder(model_key)\n",
    "    model = builder()\n",
    "    compile_model(model)\n",
    "\n",
    "    ckpt_path = MODELS_DIR / f\"{model_key}.h5\"\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_path), monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-6, verbose=1),\n",
    "    ]\n",
    "\n",
    "    t0 = time.time()\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
    "    train_time_sec = time.time() - t0\n",
    "\n",
    "    # metrics = evaluate_metrics(model, test_ds)\n",
    "    # row = {\"framework\": \"tensorflow\", \"model\": model_key, **metrics, \"train_time_sec\": train_time_sec, \"train_time_min\": train_time_sec / 60.0, \"checkpoint\": str(ckpt_path)}\n",
    "    # all_rows.append(row)\n",
    "    # print(row)\n",
    "    # ...existing code...\n",
    "    metrics = evaluate_metrics(model, test_ds)\n",
    "    row = {\"framework\": \"tensorflow\", \"model\": model_key, **metrics, \"train_time_sec\": train_time_sec, \"train_time_min\": train_time_sec / 60.0, \"checkpoint\": str(ckpt_path)}\n",
    "    all_rows.append(row)\n",
    "    print(row)\n",
    "\n",
    "    # --- new: merge/update single-row into results CSV without overwriting other entries ---\n",
    "    csv_path = RESULTS_DIR / \"tf_metrics.csv\"\n",
    "    if csv_path.exists():\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        # remove any previous entry for same (framework, model)\n",
    "        mask = ~((existing_df[\"framework\"] == row[\"framework\"]) & (existing_df[\"model\"] == row[\"model\"]))\n",
    "        existing_df = existing_df[mask]\n",
    "        combined_df = pd.concat([existing_df, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame([row])\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved/updated TensorFlow metrics -> {csv_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "# tf_df = pd.DataFrame(all_rows)\n",
    "# tf_metrics_path = RESULTS_DIR / \"tf_metrics.csv\"\n",
    "# tf_df.to_csv(tf_metrics_path, index=False)\n",
    "# print(f\"Saved TensorFlow metrics -> {tf_metrics_path}\")\n",
    "# tf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "    \n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
