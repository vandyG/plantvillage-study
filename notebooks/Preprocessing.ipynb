{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e37a8e7",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing ‚Äî Static Augmented Dataset Builder\n",
    "Build a balanced, augmented PlantVillage binary dataset (0=healthy, 1=diseased) and save RGB PNGs + CSV labels for TensorFlow and PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f028c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ RESUMING from image 045104.jpg (last was 045103.jpg)\n",
      "Loading PlantVillage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 15:29:16.259687: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-11-28 15:29:16.259709: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-11-28 15:29:16.259712: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.66 GB\n",
      "2025-11-28 15:29:16.259727: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-28 15:29:16.259737: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-11-28 15:29:20.180028: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-11-28 15:29:27.669710: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Healthy=12052, Diseased=31391, multiplier=3\n",
      "‚è≠ Skipping first 38 batches\n",
      "‚úÖ Batch 38 completed in 20.75s at 2025-11-28 15:29:52\n",
      "‚úÖ Batch 39 completed in 35.41s at 2025-11-28 15:30:27\n",
      "‚úÖ Batch 40 completed in 50.06s at 2025-11-28 15:31:17\n",
      "‚úÖ Batch 41 completed in 67.66s at 2025-11-28 15:32:25\n",
      "‚úÖ Batch 42 completed in 81.95s at 2025-11-28 15:33:47\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#   PLANTVILLAGE PREPROCESSING (JPEG) WITH RESUME + TIMING + A+B OPT\n",
    "# ======================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import csv, math\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================== SETTINGS ================================\n",
    "BATCH_SIZE = 750\n",
    "IMAGE_SIZE = (224, 224)\n",
    "SEED = 42\n",
    "\n",
    "PROFILE_MODE = False   # profile only 1 batch if True\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = Path(\"preprocessed\")\n",
    "IMG_DIR = OUTPUT_DIR / \"images\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "IMG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CSV_PATH = OUTPUT_DIR / \"labels.csv\"\n",
    "\n",
    "# ======================================================================\n",
    "# RESUME SUPPORT\n",
    "# ======================================================================\n",
    "existing_jpegs = []\n",
    "for f in IMG_DIR.glob(\"*.jpg\"):\n",
    "    nums = re.findall(r\"(\\d+)\", f.name)\n",
    "    if nums:\n",
    "        existing_jpegs.append((int(nums[0]), f.name))\n",
    "\n",
    "if existing_jpegs:\n",
    "    existing_jpegs.sort()\n",
    "    last_num, last_file = existing_jpegs[-1]\n",
    "    filename_id = last_num + 1\n",
    "    resume_mode = True\n",
    "    csv_mode = \"a\"\n",
    "    print(f\"üîÅ RESUMING from image {filename_id:06d}.jpg (last was {last_file})\")\n",
    "else:\n",
    "    filename_id = 1\n",
    "    resume_mode = False\n",
    "    csv_mode = \"w\"\n",
    "    print(\"üÜï STARTING NEW PREPROCESSING RUN ‚Äî no existing numbered JPEGs found\")\n",
    "\n",
    "# ======================================================================\n",
    "# LOAD DATASET\n",
    "# ======================================================================\n",
    "print(\"Loading PlantVillage...\")\n",
    "data, info = tfds.load(\"plant_village\", as_supervised=True, with_info=True)\n",
    "full_dataset = data[\"train\"]\n",
    "\n",
    "# binary mapping\n",
    "label_names = info.features[\"label\"].names\n",
    "binary_lookup = np.array(\n",
    "    [0 if name.split(\"___\",1)[-1].lower()==\"healthy\" else 1 for name in label_names],\n",
    "    dtype=np.int32\n",
    ")\n",
    "binary_lookup_tf = tf.constant(binary_lookup)\n",
    "\n",
    "def to_binary(img, lbl):\n",
    "    return img, tf.gather(binary_lookup_tf, lbl)\n",
    "\n",
    "full_dataset = full_dataset.map(to_binary, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# ======================================================================\n",
    "# MANUAL 80/10/10 SPLIT\n",
    "# ======================================================================\n",
    "total_count = sum(1 for _ in tfds.as_numpy(full_dataset))\n",
    "\n",
    "val_size  = int(total_count * 0.10)\n",
    "test_size = int(total_count * 0.10)\n",
    "train_size = total_count - val_size - test_size\n",
    "\n",
    "full_dataset = full_dataset.shuffle(total_count, seed=SEED, reshuffle_each_iteration=False)\n",
    "\n",
    "val_raw   = full_dataset.take(val_size)\n",
    "test_raw  = full_dataset.skip(val_size).take(test_size)\n",
    "train_raw = full_dataset.skip(val_size + test_size)\n",
    "\n",
    "train_raw = train_raw.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ======================================================================\n",
    "# COUNT HEALTHY/DISEASED IN TRAIN\n",
    "# ======================================================================\n",
    "H = 0\n",
    "D = 0\n",
    "for _, lbl in tfds.as_numpy(train_raw):\n",
    "    if lbl == 0: H += 1\n",
    "    else:        D += 1\n",
    "\n",
    "healthy_mult = math.ceil(D / H)\n",
    "print(f\"Train Healthy={H}, Diseased={D}, multiplier={healthy_mult}\")\n",
    "\n",
    "# ======================================================================\n",
    "# AUGMENT (A+B OPTIMIZED)\n",
    "# ======================================================================\n",
    "def aug_healthy(img):\n",
    "    x = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    x = tf.image.random_flip_up_down(x)\n",
    "    x = tf.image.rot90(x, tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "    x = tf.image.random_saturation(x, 0.8, 1.25)\n",
    "\n",
    "    # FAST hue (A+B)\n",
    "    hue_delta = tf.random.uniform([], -0.05, 0.05)\n",
    "    x = tf.image.adjust_hue(x, hue_delta)\n",
    "\n",
    "    x = tf.image.random_brightness(x, 0.12)\n",
    "    x = tf.image.random_contrast(x, 0.8, 1.25)\n",
    "    return tf.image.convert_image_dtype(tf.clip_by_value(x, 0, 1), tf.uint8)\n",
    "\n",
    "def aug_diseased(img):\n",
    "    x = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    x = tf.image.rot90(x, tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "    x = tf.image.random_contrast(x, 0.9, 1.1)\n",
    "\n",
    "    hue_delta = tf.random.uniform([], -0.03, 0.03)\n",
    "    x = tf.image.adjust_hue(x, hue_delta)\n",
    "\n",
    "    x = tf.image.random_brightness(x, 0.08)\n",
    "    return tf.image.convert_image_dtype(tf.clip_by_value(x, 0, 1), tf.uint8)\n",
    "\n",
    "# ======================================================================\n",
    "# BATCH RESIZE\n",
    "# ======================================================================\n",
    "def resize_batch(batch_uint8):\n",
    "    batch_f = tf.image.convert_image_dtype(batch_uint8, tf.float32)\n",
    "    batch_r = tf.image.resize(batch_f, IMAGE_SIZE)\n",
    "    batch_r = tf.image.convert_image_dtype(batch_r, tf.uint8)\n",
    "    return batch_r.numpy()\n",
    "\n",
    "# ======================================================================\n",
    "# JPEG SAVE\n",
    "# ======================================================================\n",
    "def save_jpeg(arr, filename):\n",
    "    Image.fromarray(arr).save(IMG_DIR / filename, format=\"JPEG\", quality=95)\n",
    "\n",
    "# ======================================================================\n",
    "# CSV HEADER\n",
    "# ======================================================================\n",
    "with open(CSV_PATH, csv_mode, newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not resume_mode:\n",
    "        writer.writerow([\"filename\", \"label\", \"split\"])\n",
    "\n",
    "# ======================================================================\n",
    "# SKIP COMPLETED BATCHES\n",
    "# ======================================================================\n",
    "processed_images = len(existing_jpegs)\n",
    "est_per_batch = int((H/train_size)*healthy_mult*BATCH_SIZE + (D/train_size)*BATCH_SIZE)\n",
    "skip_batches = processed_images // max(1, est_per_batch)\n",
    "print(f\"‚è≠ Skipping first {skip_batches} batches\")\n",
    "\n",
    "# ======================================================================\n",
    "# TRAIN LOOP WITH TIMING + A+B OPT\n",
    "# ======================================================================\n",
    "batch_idx = 0\n",
    "\n",
    "with open(CSV_PATH, \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for images, labels in train_raw.batch(BATCH_SIZE):\n",
    "\n",
    "        if batch_idx < skip_batches:\n",
    "            batch_idx += 1\n",
    "            continue\n",
    "\n",
    "        # START batch timer\n",
    "        batch_start = time.time()\n",
    "\n",
    "        images_np = images.numpy()\n",
    "        labels_np = labels.numpy()\n",
    "\n",
    "        # ================== PROFILING MODE ==================\n",
    "        if PROFILE_MODE:\n",
    "            t0 = time.time()\n",
    "\n",
    "            h_idx = np.where(labels_np == 0)[0]\n",
    "            d_idx = np.where(labels_np == 1)[0]\n",
    "\n",
    "            t_split = time.time()\n",
    "\n",
    "            # aug speed\n",
    "            aug_times = []\n",
    "            for img in images_np[h_idx][:20]:\n",
    "                a0 = time.time()\n",
    "                _ = aug_healthy(img)\n",
    "                aug_times.append(time.time() - a0)\n",
    "\n",
    "            # resize speed\n",
    "            r0 = time.time()\n",
    "            _ = resize_batch(images_np[:50])\n",
    "            r1 = time.time()\n",
    "\n",
    "            # jpeg save speed\n",
    "            s0 = time.time()\n",
    "            save_jpeg(images_np[0], \"profiling_test.jpg\")\n",
    "            s1 = time.time()\n",
    "\n",
    "            print(\"\\nüîç ===== PROFILING THIS BATCH =====\")\n",
    "            print(f\"Split time:       {(t_split - t0):.5f}s\")\n",
    "            print(f\"Avg augment time: {np.mean(aug_times):.5f}s\")\n",
    "            print(f\"Batch resize:     {(r1 - r0):.5f}s\")\n",
    "            print(f\"JPEG save time:   {(s1 - s0):.5f}s\")\n",
    "            print(\"========================================\\n\")\n",
    "\n",
    "            print(\"‚õî Profiling ON ‚Äî stopping after one batch.\")\n",
    "            break\n",
    "\n",
    "        # ================== REAL PIPELINE ==================\n",
    "        h_idx = np.where(labels_np == 0)[0]\n",
    "        d_idx = np.where(labels_np == 1)[0]\n",
    "\n",
    "        # ---- HEALTHY (replicated) ----\n",
    "        for rep in range(healthy_mult):\n",
    "            outs = []\n",
    "            for img in images_np[h_idx]:\n",
    "                out = aug_healthy(img) if rep > 0 else img\n",
    "                outs.append(out)\n",
    "\n",
    "            outs = resize_batch(np.stack(outs))\n",
    "\n",
    "            for out in outs:\n",
    "                fname = f\"{filename_id:06d}.jpg\"\n",
    "                save_jpeg(out, fname)\n",
    "                writer.writerow([fname, 0, \"train\"])\n",
    "                filename_id += 1\n",
    "\n",
    "        # ---- DISEASED ----\n",
    "        outs = []\n",
    "        for img in images_np[d_idx]:\n",
    "            out = aug_diseased(img) if np.random.rand() > 0.5 else img\n",
    "            outs.append(out)\n",
    "\n",
    "        outs = resize_batch(np.stack(outs))\n",
    "\n",
    "        for out in outs:\n",
    "            fname = f\"{filename_id:06d}.jpg\"\n",
    "            save_jpeg(out, fname)\n",
    "            writer.writerow([fname, 1, \"train\"])\n",
    "            filename_id += 1\n",
    "\n",
    "        # END batch timer\n",
    "        batch_end = time.time()\n",
    "        elapsed = batch_end - batch_start\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        print(f\"‚úÖ Batch {batch_idx} completed in {elapsed:.2f}s at {now}\")\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "print(\"\\nTRAIN DONE (profiling or resume).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BASIC INFO =====\n",
      "     filename  label  split\n",
      "0  000001.jpg      0  train\n",
      "1  000002.jpg      0  train\n",
      "2  000003.jpg      0  train\n",
      "3  000004.jpg      0  train\n",
      "4  000005.jpg      0  train\n",
      "\n",
      "Total images: 23228\n",
      "\n",
      "Counts per split:\n",
      "split\n",
      "train    23228\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts per label:\n",
      "label\n",
      "healthy     12795\n",
      "diseased    10433\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Counts per split per label:\n",
      "split  label\n",
      "train  0        12795\n",
      "       1        10433\n",
      "dtype: int64\n",
      "\n",
      "===== SUMMARY TABLE =====\n",
      "       healthy  diseased  total\n",
      "split                          \n",
      "train    12795     10433  23228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"===== BASIC INFO =====\")\n",
    "print(df.head())\n",
    "print(f\"\\nTotal images: {len(df)}\")\n",
    "\n",
    "# Unique splits\n",
    "print(\"\\nCounts per split:\")\n",
    "print(df['split'].value_counts())\n",
    "\n",
    "# Unique labels\n",
    "print(\"\\nCounts per label:\")\n",
    "label_map = {0: \"healthy\", 1: \"diseased\"}\n",
    "print(df['label'].map(label_map).value_counts())\n",
    "\n",
    "# Split + label combination\n",
    "print(\"\\nCounts per split per label:\")\n",
    "print(df.groupby(['split', 'label']).size())\n",
    "\n",
    "# Summary table\n",
    "summary = df.groupby(['split', 'label']).size().unstack(fill_value=0)\n",
    "summary.columns = [\"healthy\", \"diseased\"]\n",
    "summary[\"total\"] = summary.sum(axis=1)\n",
    "\n",
    "print(\"\\n===== SUMMARY TABLE =====\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb70b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
