{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73a8166",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 788\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNo models were successfully tested.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 788\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 752\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    745\u001b[0m config \u001b[38;5;241m=\u001b[39m TestConfig(\n\u001b[1;32m    746\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    747\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    748\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    749\u001b[0m )\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# Paths - Updated for current workspace structure\u001b[39;00m\n\u001b[0;32m--> 752\u001b[0m workspace_root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent  \u001b[38;5;66;03m# Go up from notebooks/ to project root\u001b[39;00m\n\u001b[1;32m    753\u001b[0m models_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(workspace_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    754\u001b[0m dataset_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(workspace_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplantdoc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test Trained Models on PlantDoc Dataset - MPS Optimized\n",
    "========================================================\n",
    "Tests previously trained binary classification models on the PlantDoc dataset.\n",
    "Optimized for Apple Silicon (MPS), CUDA, and CPU devices.\n",
    "\n",
    "The PlantDoc dataset contains multi-class labels (different diseases), but our models\n",
    "perform binary classification (healthy vs diseased). This script:\n",
    "1. Loads the PlantDoc dataset and converts to binary labels\n",
    "2. Tests all trained models from the notebooks/models/ directory\n",
    "3. Provides comprehensive comparison metrics\n",
    "4. Generates detailed visualizations\n",
    "\n",
    "Binary Label Mapping:\n",
    "- \"healthy\" keywords in folder name → 0 (healthy)\n",
    "- Any disease name in folder name → 1 (diseased)\n",
    "\n",
    "Usage:\n",
    "    python test_plant_doc_mps.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration and Data Classes\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TestConfig:\n",
    "    \"\"\"Configuration for model testing.\"\"\"\n",
    "    batch_size: int = 32\n",
    "    # Support MPS (Apple Silicon), CUDA, and CPU\n",
    "    device: str = (\n",
    "        \"cuda\" if torch.cuda.is_available() \n",
    "        else \"mps\" if torch.backends.mps.is_available() \n",
    "        else \"cpu\"\n",
    "    )\n",
    "    num_workers: int = 4\n",
    "    pin_memory: bool = True\n",
    "    use_amp: bool = False  # Mixed precision inference (CUDA only)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestMetrics:\n",
    "    \"\"\"Metrics for a single model on test set.\"\"\"\n",
    "    model_name: str\n",
    "    model_path: str\n",
    "    inference_time: float\n",
    "    samples_per_second: float\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    auc_roc: float\n",
    "    confusion_matrix: List[List[int]]\n",
    "    classification_report: str\n",
    "    # ROC curve data\n",
    "    roc_fpr: List[float]\n",
    "    roc_tpr: List[float]\n",
    "    roc_thresholds: List[float]\n",
    "    # Per-class metrics\n",
    "    true_positives: int\n",
    "    true_negatives: int\n",
    "    false_positives: int\n",
    "    false_negatives: int\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset - PlantDoc with Binary Labels\n",
    "# ============================================================================\n",
    "\n",
    "class PlantDocBinaryDataset(Dataset):\n",
    "    \"\"\"PlantDoc dataset with binary classification (healthy vs diseased).\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir: str, split: str = \"test\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Root directory of PlantDoc dataset\n",
    "            split: 'train' or 'test'\n",
    "            transform: Optional transform to apply to images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir) / split\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_names = []\n",
    "        \n",
    "        # Load all images and create binary labels\n",
    "        for class_dir in sorted(self.root_dir.iterdir()):\n",
    "            if not class_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            class_name = class_dir.name\n",
    "            self.class_names.append(class_name)\n",
    "            \n",
    "            # Determine binary label: 0 for healthy, 1 for diseased\n",
    "            # Folders with \"leaf\" without disease indicators are healthy\n",
    "            # All others are diseased\n",
    "            is_healthy = self._is_healthy_class(class_name)\n",
    "            binary_label = 0 if is_healthy else 1\n",
    "            \n",
    "            # Load all images in this class\n",
    "            for img_path in class_dir.glob(\"*\"):\n",
    "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                    self.samples.append((str(img_path), binary_label, class_name))\n",
    "        \n",
    "        print(f\"\\nPlantDoc {split} dataset loaded:\")\n",
    "        print(f\"  Total samples: {len(self.samples)}\")\n",
    "        \n",
    "        # Count samples per binary class\n",
    "        healthy_count = sum(1 for _, label, _ in self.samples if label == 0)\n",
    "        diseased_count = sum(1 for _, label, _ in self.samples if label == 1)\n",
    "        print(f\"  Healthy samples: {healthy_count}\")\n",
    "        print(f\"  Diseased samples: {diseased_count}\")\n",
    "        print(f\"  Unique original classes: {len(self.class_names)}\")\n",
    "    \n",
    "    def _is_healthy_class(self, class_name: str) -> bool:\n",
    "        \"\"\"Determine if a class represents healthy plants.\"\"\"\n",
    "        class_lower = class_name.lower()\n",
    "        \n",
    "        # Keywords that indicate disease\n",
    "        disease_keywords = [\n",
    "            'rust', 'scab', 'spot', 'blight', 'rot', 'mildew', \n",
    "            'bacterial', 'virus', 'mosaic', 'mold', 'septoria',\n",
    "            'spider', 'mites', 'early', 'late', 'gray', 'yellow'\n",
    "        ]\n",
    "        \n",
    "        # If any disease keyword is present, it's diseased\n",
    "        if any(keyword in class_lower for keyword in disease_keywords):\n",
    "            return False\n",
    "        \n",
    "        # Classes with just plant name and \"leaf\" are healthy\n",
    "        # e.g., \"Apple leaf\", \"Tomato leaf\", \"Cherry leaf\"\n",
    "        if 'leaf' in class_lower:\n",
    "            # Check if it's a simple \"X leaf\" pattern\n",
    "            parts = class_lower.split()\n",
    "            if len(parts) == 2 and parts[1] == 'leaf':\n",
    "                return True\n",
    "        \n",
    "        # Default to diseased if unsure\n",
    "        return False\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, class_name = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Model Loading\n",
    "# ============================================================================\n",
    "\n",
    "def load_trained_model(model_path: str, device: str) -> Tuple[nn.Module, str]:\n",
    "    \"\"\"\n",
    "    Load a trained model from checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, model_name)\n",
    "    \"\"\"\n",
    "    # Extract model name from filename (e.g., \"alexnet_best.pth\" -> \"alexnet\")\n",
    "    filename = os.path.basename(model_path)\n",
    "    model_name = filename.replace('_best.pth', '').replace('.pth', '')\n",
    "    \n",
    "    print(f\"\\nLoading model: {model_name} from {filename}\")\n",
    "    \n",
    "    # Create model architecture based on name (num_classes=1 for binary classification with BCEWithLogitsLoss)\n",
    "    from torchvision import models\n",
    "    \n",
    "    lname = model_name.lower()\n",
    "    \n",
    "    if lname == \"resnet50\":\n",
    "        model = models.resnet50(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    elif lname == \"alexnet\":\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.classifier[6] = nn.Linear(4096, 1)\n",
    "    elif lname == \"vgg16\":\n",
    "        model = models.vgg16(weights=None)\n",
    "        model.classifier[6] = nn.Linear(4096, 1)\n",
    "    elif lname == \"densenet121\":\n",
    "        model = models.densenet121(weights=None)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "    elif lname == \"googlenet\":\n",
    "        model = models.googlenet(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "        model.aux_logits = False\n",
    "    elif lname == \"shufflenet_v2\":\n",
    "        model = models.shufflenet_v2_x1_0(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    elif lname == \"mobilenet_v2\":\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "    elif lname == \"squeezenet1_0\":\n",
    "        model = models.squeezenet1_0(weights=None)\n",
    "        model.classifier[1] = nn.Conv2d(512, 1, kernel_size=1)\n",
    "        model.num_classes = 1\n",
    "    elif lname == \"mnasnet1_0\":\n",
    "        model = models.mnasnet1_0(weights=None)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "    elif \"efficientnet\" in lname or \"inception\" in lname or \"xception\" in lname:\n",
    "        # Use timm for other models\n",
    "        model = timm.create_model(lname, pretrained=False, num_classes=1)\n",
    "    else:\n",
    "        # Try timm as fallback\n",
    "        try:\n",
    "            model = timm.create_model(lname, pretrained=False, num_classes=1)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Unknown model: {model_name}. Error: {e}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        # Checkpoint saved with metadata\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"  ✓ Loaded checkpoint with metadata (trained for {checkpoint.get('train_time', 'unknown')} seconds)\")\n",
    "    else:\n",
    "        # Checkpoint is just the state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"  ✓ Model loaded successfully\")\n",
    "    \n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    device: str,\n",
    "    use_amp: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (y_true, y_pred, y_probs, inference_time)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Mixed precision inference (only supported on CUDA)\n",
    "            # MPS and CPU don't support torch.amp.autocast well, so disable for those\n",
    "            use_amp_inference = use_amp and device == 'cuda'\n",
    "            \n",
    "            if use_amp_inference:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    logits = model(images).squeeze(1)\n",
    "                    probs = torch.sigmoid(logits)\n",
    "            else:\n",
    "                logits = model(images).squeeze(1)\n",
    "                probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # Binary classification: threshold at 0.5\n",
    "            predicted = (probs > 0.5).long()\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return (\n",
    "        np.array(y_true),\n",
    "        np.array(y_pred),\n",
    "        np.array(y_probs),\n",
    "        inference_time\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_probs: np.ndarray,\n",
    "    model_name: str,\n",
    "    model_path: str,\n",
    "    inference_time: float,\n",
    "    num_samples: int,\n",
    ") -> TestMetrics:\n",
    "    \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "    \n",
    "    # Validate and clean y_probs\n",
    "    if np.any(np.isnan(y_probs)) or np.any(np.isinf(y_probs)):\n",
    "        print(\"  Warning: Invalid probabilities detected, clipping to [0, 1]\")\n",
    "        y_probs = np.nan_to_num(y_probs, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "        y_probs = np.clip(y_probs, 0, 1)\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "    \n",
    "    # Compute confusion matrix components\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Classification report\n",
    "    class_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=['Healthy', 'Diseased'],\n",
    "        digits=4\n",
    "    )\n",
    "    \n",
    "    return TestMetrics(\n",
    "        model_name=model_name,\n",
    "        model_path=model_path,\n",
    "        inference_time=inference_time,\n",
    "        samples_per_second=num_samples / inference_time if inference_time > 0 else 0,\n",
    "        accuracy=accuracy_score(y_true, y_pred),\n",
    "        precision=precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1_score=f1_score(y_true, y_pred, zero_division=0),\n",
    "        auc_roc=roc_auc_score(y_true, y_probs),\n",
    "        confusion_matrix=cm.tolist(),\n",
    "        classification_report=class_report,\n",
    "        roc_fpr=fpr.tolist(),\n",
    "        roc_tpr=tpr.tolist(),\n",
    "        roc_thresholds=thresholds.tolist(),\n",
    "        true_positives=int(tp),\n",
    "        true_negatives=int(tn),\n",
    "        false_positives=int(fp),\n",
    "        false_negatives=int(fn),\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Testing Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def test_all_models(\n",
    "    models_dir: str,\n",
    "    dataset_root: str,\n",
    "    config: TestConfig,\n",
    "    output_dir: str = \"data/output\"\n",
    ") -> List[TestMetrics]:\n",
    "    \"\"\"\n",
    "    Test all trained models on PlantDoc dataset.\n",
    "    \n",
    "    Args:\n",
    "        models_dir: Directory containing trained model checkpoints\n",
    "        dataset_root: Root directory of PlantDoc dataset\n",
    "        config: Test configuration\n",
    "        output_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        List of TestMetrics for each model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING TRAINED MODELS ON PLANTDOC DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Device: {config.device.upper()}\")\n",
    "    if config.device == \"mps\":\n",
    "        print(\"  (Apple Silicon GPU - Metal Performance Shaders)\")\n",
    "    elif config.device == \"cuda\":\n",
    "        print(f\"  (NVIDIA GPU: {torch.cuda.get_device_name(0)})\")\n",
    "    else:\n",
    "        print(\"  (CPU)\")\n",
    "    print(f\"Batch Size: {config.batch_size}\")\n",
    "    print(f\"Mixed Precision: {config.use_amp} (only active on CUDA)\")\n",
    "    \n",
    "    # Load PlantDoc test set\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    test_dataset = PlantDocBinaryDataset(\n",
    "        root_dir=dataset_root,\n",
    "        split=\"test\",\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    # Find all model checkpoints\n",
    "    model_paths = sorted(glob.glob(os.path.join(models_dir, \"*.pth\")))\n",
    "    \n",
    "    if not model_paths:\n",
    "        print(f\"\\nError: No model checkpoints found in {models_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nFound {len(model_paths)} model checkpoint(s):\")\n",
    "    for path in model_paths:\n",
    "        print(f\"  - {os.path.basename(path)}\")\n",
    "    \n",
    "    # Test each model\n",
    "    results = []\n",
    "    for model_path in model_paths:\n",
    "        try:\n",
    "            print(f\"\\n{'#'*70}\")\n",
    "            print(f\"# Testing {os.path.basename(model_path)}\")\n",
    "            print(f\"{'#'*70}\")\n",
    "            \n",
    "            # Load model\n",
    "            model, model_name = load_trained_model(model_path, config.device)\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"Running inference on test set...\")\n",
    "            y_true, y_pred, y_probs, inference_time = evaluate_model(\n",
    "                model, test_loader, config.device, use_amp=config.use_amp\n",
    "            )\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics = compute_metrics(\n",
    "                y_true, y_pred, y_probs,\n",
    "                model_name, model_path,\n",
    "                inference_time, len(test_dataset)\n",
    "            )\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\n{model_name} Results:\")\n",
    "            print(f\"  Accuracy:  {metrics.accuracy:.4f}\")\n",
    "            print(f\"  Precision: {metrics.precision:.4f}\")\n",
    "            print(f\"  Recall:    {metrics.recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {metrics.f1_score:.4f}\")\n",
    "            print(f\"  AUC-ROC:   {metrics.auc_roc:.4f}\")\n",
    "            print(f\"  Inference Time: {metrics.inference_time:.2f}s\")\n",
    "            print(f\"  Samples/sec:    {metrics.samples_per_second:.1f}\")\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(f\"  TN: {metrics.true_negatives:4d}  FP: {metrics.false_positives:4d}\")\n",
    "            print(f\"  FN: {metrics.false_negatives:4d}  TP: {metrics.true_positives:4d}\")\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            # Cleanup\n",
    "            del model\n",
    "            if config.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            elif config.device == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError testing {os.path.basename(model_path)}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def plot_metrics_comparison(results: List[TestMetrics], save_path: str = None):\n",
    "    \"\"\"Plot comparison of key metrics across models.\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    model_names = [r.model_name for r in results]\n",
    "    metrics_to_plot = {\n",
    "        'Accuracy': [r.accuracy for r in results],\n",
    "        'Precision': [r.precision for r in results],\n",
    "        'Recall': [r.recall for r in results],\n",
    "        'F1-Score': [r.f1_score for r in results],\n",
    "        'AUC-ROC': [r.auc_roc for r in results],\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.15\n",
    "    multiplier = 0\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(metrics_to_plot)))\n",
    "    \n",
    "    for (metric_name, values), color in zip(metrics_to_plot.items(), colors):\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, values, width, label=metric_name, color=color)\n",
    "        ax.bar_label(rects, fmt='%.3f', padding=3, fontsize=8)\n",
    "        multiplier += 1\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('PlantDoc Test Set - Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x + width * 2)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nMetrics comparison saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrices(results: List[TestMetrics], save_path: str = None):\n",
    "    \"\"\"Plot confusion matrices for all models.\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    n_models = len(results)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, result in zip(axes, results):\n",
    "        cm = np.array(result.confusion_matrix)\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Healthy', 'Diseased'],\n",
    "            yticklabels=['Healthy', 'Diseased'],\n",
    "            cbar_kws={'label': 'Count'}\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'{result.model_name}\\nAccuracy: {result.accuracy:.4f}', \n",
    "                    fontweight='bold')\n",
    "        ax.set_ylabel('True Label')\n",
    "        ax.set_xlabel('Predicted Label')\n",
    "        \n",
    "        # Add statistics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        stats_text = f'Sensitivity: {tp/(tp+fn):.3f}\\nSpecificity: {tn/(tn+fp):.3f}'\n",
    "        ax.text(1.5, -0.15, stats_text, transform=ax.transData, \n",
    "               fontsize=9, verticalalignment='top')\n",
    "    \n",
    "    plt.suptitle('Confusion Matrices - PlantDoc Test Set', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrices saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curves(results: List[TestMetrics], save_path: str = None):\n",
    "    \"\"\"Plot ROC curves for all models.\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "    \n",
    "    for result, color in zip(results, colors):\n",
    "        ax.plot(\n",
    "            result.roc_fpr, result.roc_tpr,\n",
    "            label=f'{result.model_name} (AUC = {result.auc_roc:.4f})',\n",
    "            color=color, linewidth=2\n",
    "        )\n",
    "    \n",
    "    # Plot diagonal line (random classifier)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curves - PlantDoc Test Set', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ROC curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_inference_speed(results: List[TestMetrics], save_path: str = None):\n",
    "    \"\"\"Plot inference speed comparison.\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    model_names = [r.model_name for r in results]\n",
    "    samples_per_sec = [r.samples_per_second for r in results]\n",
    "    inference_times = [r.inference_time for r in results]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Samples per second\n",
    "    bars1 = ax1.bar(model_names, samples_per_sec, color=plt.cm.viridis(np.linspace(0, 1, len(results))))\n",
    "    ax1.set_xlabel('Model', fontsize=12)\n",
    "    ax1.set_ylabel('Samples per Second', fontsize=12)\n",
    "    ax1.set_title('Inference Speed', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars1, samples_per_sec):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Total inference time\n",
    "    bars2 = ax2.bar(model_names, inference_times, color=plt.cm.plasma(np.linspace(0, 1, len(results))))\n",
    "    ax2.set_xlabel('Model', fontsize=12)\n",
    "    ax2.set_ylabel('Total Time (seconds)', fontsize=12)\n",
    "    ax2.set_title('Total Inference Time', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars2, inference_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Inference speed comparison saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results: List[TestMetrics], output_dir: str = \"data/output\"):\n",
    "    \"\"\"Save test results to JSON and create visualizations.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save JSON results\n",
    "    json_path = os.path.join(output_dir, f\"plantdoc_test_results_{timestamp}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump([asdict(r) for r in results], f, indent=2)\n",
    "    print(f\"\\nResults saved to {json_path}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    plot_metrics_comparison(\n",
    "        results,\n",
    "        save_path=os.path.join(output_dir, f\"plantdoc_metrics_comparison_{timestamp}.png\")\n",
    "    )\n",
    "    \n",
    "    plot_confusion_matrices(\n",
    "        results,\n",
    "        save_path=os.path.join(output_dir, f\"plantdoc_confusion_matrices_{timestamp}.png\")\n",
    "    )\n",
    "    \n",
    "    plot_roc_curves(\n",
    "        results,\n",
    "        save_path=os.path.join(output_dir, f\"plantdoc_roc_curves_{timestamp}.png\")\n",
    "    )\n",
    "    \n",
    "    plot_inference_speed(\n",
    "        results,\n",
    "        save_path=os.path.join(output_dir, f\"plantdoc_inference_speed_{timestamp}.png\")\n",
    "    )\n",
    "    \n",
    "    # Create summary table\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(\"PLANTDOC TEST SET - COMPREHENSIVE SUMMARY\")\n",
    "    print(f\"{'='*90}\")\n",
    "    print(f\"{'Model':<20} {'Acc':<8} {'Prec':<8} {'Rec':<8} {'F1':<8} {'AUC':<8} {'Infer(s)':<10} {'Samp/s':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    for r in results:\n",
    "        print(f\"{r.model_name:<20} {r.accuracy:<8.4f} {r.precision:<8.4f} \"\n",
    "              f\"{r.recall:<8.4f} {r.f1_score:<8.4f} {r.auc_roc:<8.4f} \"\n",
    "              f\"{r.inference_time:<10.2f} {r.samples_per_second:<10.1f}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Print detailed classification reports\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "    print(f\"{'='*90}\")\n",
    "    for r in results:\n",
    "        print(f\"\\n{r.model_name}:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(r.classification_report)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Entry Point\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main testing pipeline.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = TestConfig(\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        use_amp=True,\n",
    "    )\n",
    "    \n",
    "    # Paths - Updated for current workspace structure\n",
    "    workspace_root = Path(__file__).parent.parent  # Go up from notebooks/ to project root\n",
    "    models_dir = str(workspace_root / \"notebooks\" / \"models\")\n",
    "    dataset_root = str(workspace_root / \"data\" / \"plantdoc\")\n",
    "    output_dir = str(workspace_root / \"data\" / \"output\")\n",
    "    \n",
    "    print(\"\\nUsing paths:\")\n",
    "    print(f\"  Models: {models_dir}\")\n",
    "    print(f\"  Dataset: {dataset_root}\")\n",
    "    print(f\"  Output: {output_dir}\")\n",
    "    \n",
    "    # Check if paths exist\n",
    "    if not os.path.exists(models_dir):\n",
    "        print(f\"Error: Models directory not found: {models_dir}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(dataset_root):\n",
    "        print(f\"Error: Dataset not found: {dataset_root}\")\n",
    "        return\n",
    "    \n",
    "    # Run tests\n",
    "    results = test_all_models(\n",
    "        models_dir=models_dir,\n",
    "        dataset_root=dataset_root,\n",
    "        config=config,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Save and visualize results\n",
    "    if results:\n",
    "        save_results(results, output_dir=output_dir)\n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(\"✓ Testing complete! All results saved.\")\n",
    "        print(f\"{'='*90}\")\n",
    "    else:\n",
    "        print(\"\\nNo models were successfully tested.\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba8d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
