{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c2b46a",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-Model Training & Profiling\n",
    "Complete binary PlantVillage comparison (MobileNetV2, EfficientNetB0, ResNet50, VGG16, DenseNet121, InceptionV3) with timing and memory stats. Optimized defaults for M3 Pro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743d5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CHECKING ACCELERATORS =====\n",
      "‚úì PyTorch running on Apple MPS GPU\n",
      "TensorFlow GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1 ‚Äî Imports, Global Utilities, Memory Tracking\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# sklearn metrics for evaluation\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "print(\"\\n===== CHECKING ACCELERATORS =====\")\n",
    "print(f\"TensorFlow GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY TRACKING HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Return RAM + GPU memory in GB. Safe on Apple Silicon.\"\"\"\n",
    "    info = {\"ram_gb\": None, \"gpu_current_gb\": None, \"gpu_peak_gb\": None}\n",
    "\n",
    "    # RAM usage (always available)\n",
    "    try:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        info[\"ram_gb\"] = process.memory_info().rss / (1024 ** 3)\n",
    "    except Exception:\n",
    "        info[\"ram_gb\"] = None\n",
    "\n",
    "    # GPU memory ‚Äî works on NVIDIA CUDA only\n",
    "    try:\n",
    "        mem_info = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "        info[\"gpu_current_gb\"] = mem_info[\"current\"] / (1024 ** 3)\n",
    "        info[\"gpu_peak_gb\"] = mem_info[\"peak\"] / (1024 ** 3)\n",
    "    except Exception:\n",
    "        # Apple Metal + some TF versions: unsupported\n",
    "        info[\"gpu_current_gb\"] = None\n",
    "        info[\"gpu_peak_gb\"] = None\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def reset_gpu_memory_stats():\n",
    "    \"\"\"Safely reset GPU memory stats (CUDA only).\"\"\"\n",
    "    try:\n",
    "        tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f00d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== STEP 2: TensorFlow Hardware Configuration =====\n",
      "Configuring TensorFlow for Apple M-Series / CUDA...\n",
      "‚úì TensorFlow detected 1 GPU(s)\n",
      "‚úì GPU memory growth enabled\n",
      "‚úì Optimized TensorFlow threading for M-Series CPU\n",
      "TensorFlow version    : 2.16.2\n",
      "Detected TF GPUs      : 1\n",
      "PyTorch device chosen : mps\n",
      "=========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2 ‚Äî Global Config + TensorFlow Hardware Setup\n",
    "# ============================================================\n",
    "\n",
    "# Global config\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "print(\"\\n===== STEP 2: TensorFlow Hardware Configuration =====\")\n",
    "print(\"Configuring TensorFlow runtime...\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GPU detection\n",
    "# -----------------------------------------------------------\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if len(gpus) > 0:\n",
    "    try:\n",
    "        # Enable memory growth (prevents OOM issues)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        print(f\"‚úì TensorFlow detected {len(gpus)} GPU(s)\")\n",
    "        print(\"‚úì GPU memory growth enabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error enabling memory growth: {e}\")\n",
    "else:\n",
    "    print(\"‚ö† No TensorFlow GPU device found ‚Äî running on CPU (slower)\")\n",
    "    print(\"  On Apple Silicon, ensure 'tensorflow-macos' + 'tensorflow-metal' are installed.\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# CPU Thread Optimization for Apple Silicon\n",
    "# -----------------------------------------------------------\n",
    "try:\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "    print(\"‚úì Optimized TensorFlow threading for Apple Silicon CPU\")\n",
    "except Exception:\n",
    "    print(\"‚ö† Threading optimization not supported on this setup\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Log TF Version / Hardware\n",
    "# -----------------------------------------------------------\n",
    "print(f\"TensorFlow version    : {tf.__version__}\")\n",
    "print(f\"Detected TF GPUs      : {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(\"=========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c0a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3 ‚Äî Load Dataset + Split + Convert to Binary Labels\n",
    "# ============================================================\n",
    "\n",
    "def load_and_split_plant_village(seed=SEED):\n",
    "    \"\"\"\n",
    "    Loads the PlantVillage dataset.\n",
    "    If only a 'train' split exists, creates train/val/test = 70/15/15.\n",
    "    Returns:\n",
    "        plant_village_data (dict of datasets)\n",
    "        info (tfds metadata)\n",
    "    \"\"\"\n",
    "    print(\"===== Loading PlantVillage Dataset =====\")\n",
    "\n",
    "    plant_village_data, info = tfds.load(\n",
    "        \"plant_village\",\n",
    "        with_info=True,\n",
    "        as_supervised=True,\n",
    "        shuffle_files=True,\n",
    "    )\n",
    "\n",
    "    # If TFDS provided only `train`, manually split it.\n",
    "    if list(plant_village_data.keys()) == [\"train\"]:\n",
    "        print(\"‚ö† Dataset only has 'train' split ‚Üí Creating 70/15/15 splits...\")\n",
    "\n",
    "        full = plant_village_data[\"train\"]\n",
    "        total_size = info.splits[\"train\"].num_examples\n",
    "\n",
    "        train_size = int(0.70 * total_size)\n",
    "        val_size = int(0.15 * total_size)\n",
    "\n",
    "        full = full.shuffle(total_size, seed=seed)\n",
    "\n",
    "        train_ds = full.take(train_size)\n",
    "        val_ds = full.skip(train_size).take(val_size)\n",
    "        test_ds = full.skip(train_size + val_size)\n",
    "\n",
    "        plant_village_data = {\n",
    "            \"train\": train_ds,\n",
    "            \"validation\": val_ds,\n",
    "            \"test\": test_ds,\n",
    "        }\n",
    "\n",
    "        print(f\"‚úì train={train_size}, val={val_size}, test={total_size - train_size - val_size}\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚úì TFDS already provides train/validation/test splits\")\n",
    "\n",
    "    print(\"=========================================\\n\")\n",
    "    return plant_village_data, info\n",
    "\n",
    "\n",
    "\n",
    "def make_binary_labels(plant_village_data, info):\n",
    "    \"\"\"\n",
    "    Converts multi-class PlantVillage labels -> binary (healthy=0, diseased=1).\n",
    "    Also calculates total counts of healthy vs diseased for augmentation strategy.\n",
    "    Returns:\n",
    "        binary_data (dict of datasets)\n",
    "        total_healthy (int)\n",
    "        total_diseased (int)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"===== Converting to Binary Labels (Healthy=0, Diseased=1) =====\")\n",
    "\n",
    "    # List of all original text labels in TFDS\n",
    "    label_names = info.features[\"label\"].names\n",
    "\n",
    "    # Build lookup: per-index ‚Üí 0/1\n",
    "    binary_lookup = np.array(\n",
    "        [0 if name.lower().endswith(\"healthy\") else 1 for name in label_names],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    binary_lookup_tf = tf.constant(binary_lookup)\n",
    "\n",
    "    def to_binary_label(image, label):\n",
    "        \"\"\"\n",
    "        Map TFDS integer label ‚Üí 0 (healthy) / 1 (diseased)\n",
    "        using the lookup table.\n",
    "        \"\"\"\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        binary_label = tf.gather(binary_lookup_tf, label)\n",
    "        return image, binary_label\n",
    "\n",
    "    # Apply to each split\n",
    "    binary_data = {\n",
    "        split: ds.map(to_binary_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        for split, ds in plant_village_data.items()\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Compute original class imbalance statistics\n",
    "    # -------------------------------------------\n",
    "    print(\"Counting healthy vs diseased samples in training split...\")\n",
    "\n",
    "    total_healthy = 0\n",
    "    total_diseased = 0\n",
    "\n",
    "    for _, label in tfds.as_numpy(plant_village_data[\"train\"]):\n",
    "        label_str = info.features[\"label\"].int2str(int(label))\n",
    "        class_name = label_str.split(\"___\", 1)[-1].lower()\n",
    "        if class_name == \"healthy\":\n",
    "            total_healthy += 1\n",
    "        else:\n",
    "            total_diseased += 1\n",
    "\n",
    "    print(f\"‚úì Healthy: {total_healthy}\")\n",
    "    print(f\"‚úì Diseased: {total_diseased}\")\n",
    "    print(f\"‚Üí Imbalance ratio: {total_diseased / max(total_healthy, 1):.2f}:1\")\n",
    "    print(\"===============================================================\\n\")\n",
    "\n",
    "    return binary_data, total_healthy, total_diseased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40b0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4 ‚Äî Augmentation + Balanced Training Dataset\n",
    "# ============================================================\n",
    "\n",
    "def build_augmented_train_ds(binary_data, total_healthy, total_diseased):\n",
    "    \"\"\"\n",
    "    Creates a balanced augmented training dataset:\n",
    "    - Aggressive augmentation for healthy class\n",
    "    - Mild augmentation / 50% replacement for diseased class\n",
    "    - Replicates healthy images to balance dataset\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"===== Building Augmented Training Dataset =====\")\n",
    "\n",
    "    train_ds = binary_data[\"train\"]\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Split into healthy vs diseased datasets\n",
    "    # ----------------------------------------------\n",
    "    healthy_label = 0\n",
    "    diseased_label = 1\n",
    "\n",
    "    healthy_ds = train_ds.filter(lambda _, lbl: tf.equal(lbl, healthy_label))\n",
    "    diseased_ds = train_ds.filter(lambda _, lbl: tf.equal(lbl, diseased_label))\n",
    "\n",
    "    print(f\"Original counts ‚Üí Healthy={total_healthy}, Diseased={total_diseased}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # AUGMENTATION FUNCTIONS\n",
    "    # ============================================================\n",
    "\n",
    "    def augment_healthy(image, label):\n",
    "        \"\"\"Strong augmentation to create variety for minority class.\"\"\"\n",
    "        img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.rot90(img, tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        img = tf.image.random_saturation(img, 0.8, 1.25)\n",
    "        img = tf.image.random_hue(img, 0.05)\n",
    "        img = tf.image.random_brightness(img, 0.12)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.25)\n",
    "\n",
    "        img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "        return tf.image.convert_image_dtype(img, tf.uint8), label\n",
    "\n",
    "\n",
    "    def augment_diseased_with_replacement(image, label):\n",
    "        \"\"\"Mild augmentations, 50% chance to keep original.\"\"\"\n",
    "        img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "        def apply_aug():\n",
    "            aug = tf.image.random_flip_left_right(img)\n",
    "            aug = tf.image.rot90(aug, tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "            aug = tf.image.random_contrast(aug, 0.9, 1.1)\n",
    "            aug = tf.image.random_brightness(aug, 0.08)\n",
    "            aug = tf.image.random_hue(aug, 0.03)\n",
    "            aug = tf.clip_by_value(aug, 0.0, 1.0)\n",
    "            return tf.image.convert_image_dtype(aug, tf.uint8)\n",
    "\n",
    "        # 50% chance original / 50% chance augmented\n",
    "        prob = tf.random.uniform([], 0.0, 1.0)\n",
    "        chosen = tf.cond(prob > 0.5, apply_aug, lambda: image)\n",
    "        return chosen, label\n",
    "\n",
    "    # ============================================================\n",
    "    # OVERSAMPLING / REPLICATION FOR HEALTHY CLASS\n",
    "    # ============================================================\n",
    "\n",
    "    # Compute replication factor\n",
    "    if total_healthy == 0:\n",
    "        healthy_multiplier = 1\n",
    "    else:\n",
    "        healthy_multiplier = max(1, math.ceil(total_diseased / total_healthy) - 1)\n",
    "\n",
    "    print(f\"Healthy class replication multiplier: {healthy_multiplier}x\")\n",
    "\n",
    "    # Apply augmentations\n",
    "    augmented_healthy_datasets = [healthy_ds]  # original included\n",
    "\n",
    "    for _ in range(healthy_multiplier):\n",
    "        augmented_healthy_datasets.append(\n",
    "            healthy_ds.map(augment_healthy, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "    # Concatenate all healthy datasets\n",
    "    healthy_augmented_ds = augmented_healthy_datasets[0]\n",
    "    for ds in augmented_healthy_datasets[1:]:\n",
    "        healthy_augmented_ds = healthy_augmented_ds.concatenate(ds)\n",
    "\n",
    "    healthy_augmented_ds = healthy_augmented_ds.shuffle(4096)\n",
    "\n",
    "    # ============================================================\n",
    "    # AUGMENT DISEASED CLASS\n",
    "    # ============================================================\n",
    "\n",
    "    diseased_augmented_ds = diseased_ds.map(\n",
    "        augment_diseased_with_replacement, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # COMBINE + SHUFFLE + PREFETCH\n",
    "    # ============================================================\n",
    "\n",
    "    augmented_train_ds = (\n",
    "        healthy_augmented_ds\n",
    "        .concatenate(diseased_augmented_ds)\n",
    "        .shuffle(8192)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    new_healthy = total_healthy * (healthy_multiplier + 1)\n",
    "    print(f\"Augmented training ‚Üí Healthy‚âà{new_healthy}, Diseased‚âà{total_diseased}\")\n",
    "    print(\"Final ratio ‚âà 1:1\")\n",
    "    print(\"===============================================================\\n\")\n",
    "\n",
    "    # Update dict\n",
    "    binary_data[\"train\"] = augmented_train_ds\n",
    "    return binary_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45153e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5 ‚Äî TF Dataset Preprocessing + Batched Pipelines\n",
    "# ============================================================\n",
    "\n",
    "def prepare_dataset(ds, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Takes a raw TF dataset (image,label) and applies:\n",
    "      - resizing to image_size\n",
    "      - scaling to [0,1]\n",
    "      - batching\n",
    "      - prefetching\n",
    "    Returns:\n",
    "        A performant tf.data.Dataset for training or evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    def preprocess(image, label):\n",
    "        # Resize all images to the chosen model input size (224x224)\n",
    "        image = tf.image.resize(image, image_size)\n",
    "        # Normalize to [0, 1] float32\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image, label\n",
    "\n",
    "    return (\n",
    "        ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_all_splits(binary_data):\n",
    "    \"\"\"\n",
    "    Wraps prepare_dataset() for all three splits.\n",
    "    Ensures consistent preprocessing for:\n",
    "        - train_ds\n",
    "        - val_ds\n",
    "        - test_ds\n",
    "    Returns:\n",
    "        (train_ds, val_ds, test_ds)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"===== Preparing TF Datasets (Batched + Prefetched) =====\")\n",
    "\n",
    "    train_ds = prepare_dataset(binary_data[\"train\"])\n",
    "    val_ds   = prepare_dataset(binary_data[\"validation\"])\n",
    "    test_ds  = prepare_dataset(binary_data[\"test\"])\n",
    "\n",
    "    print(f\"‚úì Batch size    : {BATCH_SIZE}\")\n",
    "    print(f\"‚úì Image size    : {IMAGE_SIZE}\")\n",
    "    print(\"‚úì train/val/test datasets ready\")\n",
    "    print(\"=========================================================\\n\")\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c94b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6 ‚Äî Model Builders (TensorFlow)\n",
    "# ============================================================\n",
    "\n",
    "from tensorflow.keras.applications import (\n",
    "    MobileNetV2,\n",
    "    EfficientNetB0,\n",
    "    VGG16,\n",
    "    ResNet50,\n",
    "    Xception,\n",
    ")\n",
    "\n",
    "TF_MODEL_REGISTRY = {\n",
    "    \"mobilenet_v2\": MobileNetV2,\n",
    "    \"efficientnet_b0\": EfficientNetB0,\n",
    "    \"vgg16\": VGG16,\n",
    "    \"resnet50\": ResNet50,\n",
    "    \"xception\": Xception,\n",
    "}\n",
    "\n",
    "\n",
    "def create_binary_classifier(base_model_fn, input_shape=(224, 224, 3), dropout=0.2):\n",
    "    base_model = base_model_fn(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = base_model.input\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_builder(model_key):\n",
    "    key = model_key.lower()\n",
    "    if key not in TF_MODEL_REGISTRY:\n",
    "        raise ValueError(f\"Unknown TensorFlow model key: {model_key}\")\n",
    "\n",
    "    def builder():\n",
    "        return create_binary_classifier(TF_MODEL_REGISTRY[key])\n",
    "\n",
    "    return builder\n",
    "\n",
    "\n",
    "MODELS_TO_TRAIN = [\n",
    "    \"mobilenet_v2\",\n",
    "    \"efficientnet_b0\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e045c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7 ‚Äî TensorFlow Training Utilities\n",
    "# ============================================================\n",
    "\n",
    "def compile_tf_model(model, lr=1e-3):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.AUC(name=\"auc\"),\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_tf_model(model, train_ds, val_ds, ckpt_path, log_dir, epochs=10):\n",
    "    compile_tf_model(model)\n",
    "\n",
    "    cb = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            ckpt_path, monitor=\"val_auc\", mode=\"max\", save_best_only=True\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=cb,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def train_single_model(model_key, train_ds, val_ds, epochs=EPOCHS):\n",
    "    print(f\"\\n===== TRAINING MODEL: {model_key} =====\")\n",
    "\n",
    "    builder = get_model_builder(model_key)\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    ckpt_path = f\"models/{model_key}_{timestamp}.keras\"\n",
    "    log_dir = f\"logs/{model_key}_{timestamp}\"\n",
    "\n",
    "    reset_gpu_memory_stats()\n",
    "    mem_before = get_memory_usage()\n",
    "    t0 = time.time()\n",
    "\n",
    "    model = builder()\n",
    "    num_params = model.count_params()\n",
    "\n",
    "    model, history = train_tf_model(\n",
    "        model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        ckpt_path=ckpt_path,\n",
    "        log_dir=log_dir,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "    mem_after = get_memory_usage()\n",
    "\n",
    "    print(f\"‚úì Model trained: {model_key}\")\n",
    "    print(f\"‚Üí Params: {num_params:,}\")\n",
    "    print(f\"‚Üí Training time: {train_time:.2f} sec ({train_time/60:.2f} min)\")\n",
    "\n",
    "    return model, history, ckpt_path, num_params, train_time, mem_before, mem_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f9430",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 8 ‚Äî Evaluation & End-to-End Orchestration (TensorFlow)\n",
    "# ============================================================\n",
    "\n",
    "def predict_tf(model, test_ds):\n",
    "    y_true, y_pred, y_proba = [], [], []\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        probs = model.predict(images, verbose=0).flatten()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds)\n",
    "        y_proba.extend(probs)\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred), np.array(y_proba)\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    acc = (y_true == y_pred).mean()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    return {\n",
    "        \"test_accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"report\": classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=[\"Healthy\", \"Diseased\"],\n",
    "            zero_division=0,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, model_key):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Healthy\", \"Diseased\"],\n",
    "        yticklabels=[\"Healthy\", \"Diseased\"],\n",
    "    )\n",
    "    plt.title(f\"{model_key} ‚Äî Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "\n",
    "    out_path = f\"models/{model_key}_cm.png\"\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_ds, model_key):\n",
    "    print(f\"‚Üí Evaluating: {model_key}\")\n",
    "\n",
    "    y_true, y_pred, y_proba = predict_tf(model, test_ds)\n",
    "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
    "    metrics[\"cm_path\"] = save_confusion_matrix(y_true, y_pred, model_key)\n",
    "\n",
    "    print(\n",
    "        f\"{model_key} ‚Üí ACC={metrics['test_accuracy']:.4f}, \"\n",
    "        f\"PREC={metrics['precision']:.4f}, \"\n",
    "        f\"REC={metrics['recall']:.4f}, \"\n",
    "        f\"F1={metrics['f1']:.4f}, \"\n",
    "        f\"AUC={metrics['auc']:.4f}\"\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_all_models(models=MODELS_TO_TRAIN, epochs=EPOCHS):\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"===== MODELS TO TRAIN =====\")\n",
    "    for i, m in enumerate(models, start=1):\n",
    "        print(f\"{i}. {m}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Load + preprocess dataset\n",
    "    # --------------------------------------------------------\n",
    "    plant_village_data, info = load_and_split_plant_village()\n",
    "    binary_data, total_healthy, total_diseased = make_binary_labels(plant_village_data, info)\n",
    "    binary_data = build_augmented_train_ds(binary_data, total_healthy, total_diseased)\n",
    "    train_ds, val_ds, test_ds = prepare_all_splits(binary_data)\n",
    "\n",
    "    all_rows = []\n",
    "    histories = {}\n",
    "\n",
    "    def flatten(prefix, mem):\n",
    "        return {\n",
    "            f\"{prefix}_ram_gb\": mem[\"ram_gb\"],\n",
    "            f\"{prefix}_gpu_current_gb\": mem[\"gpu_current_gb\"],\n",
    "            f\"{prefix}_gpu_peak_gb\": mem[\"gpu_peak_gb\"],\n",
    "        }\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Loop through models\n",
    "    # --------------------------------------------------------\n",
    "    for model_key in models:\n",
    "        print(\"\\n====================================================\")\n",
    "        print(f\"üöÄ Starting training for: {model_key}\")\n",
    "        print(\"====================================================\")\n",
    "\n",
    "        model, history, ckpt_path, params, train_time, mem_before, mem_after = train_single_model(\n",
    "            model_key,\n",
    "            train_ds,\n",
    "            val_ds,\n",
    "            epochs,\n",
    "        )\n",
    "\n",
    "        print(f\"üîç Evaluating model: {model_key}\")\n",
    "\n",
    "        metrics = evaluate_model(\n",
    "            model,\n",
    "            test_ds=test_ds,\n",
    "            model_key=model_key,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"model\": model_key,\n",
    "            \"params\": params,\n",
    "            \"train_time_sec\": train_time,\n",
    "            \"train_time_min\": train_time / 60,\n",
    "            \"checkpoint_path\": ckpt_path,\n",
    "            \"classification_report\": metrics.pop(\"report\"),\n",
    "        }\n",
    "        row.update(flatten(\"mem_before\", mem_before))\n",
    "        row.update(flatten(\"mem_after\", mem_after))\n",
    "        row.update(metrics)\n",
    "\n",
    "        all_rows.append(row)\n",
    "        histories[model_key] = history\n",
    "\n",
    "        print(f\"‚úî Finished model: {model_key}\")\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df.to_csv(\"results/model_comparison.csv\", index=False)\n",
    "\n",
    "    print(\"\\n===== ALL MODELS COMPLETED SUCCESSFULLY =====\")\n",
    "    print(\"üìÅ Results saved to results/model_comparison.csv\\n\")\n",
    "\n",
    "    return df, histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12ce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "===== MODELS TO TRAIN =====\n",
      "1. alexnet\n",
      "==============================\n",
      "\n",
      "===== Loading PlantVillage Dataset =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:34:05.618687: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-11-27 15:34:05.618724: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-11-27 15:34:05.618730: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.66 GB\n",
      "2025-11-27 15:34:05.618763: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-27 15:34:05.618774: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Dataset only has 'train' split ‚Üí Creating 70/15/15 splits...\n",
      "‚úì train=38012, val=8145, test=8146\n",
      "=========================================\n",
      "\n",
      "===== Converting to Binary Labels (Healthy=0, Diseased=1) =====\n",
      "Counting healthy vs diseased samples in training split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:34:13.113485: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Healthy: 10465\n",
      "‚úì Diseased: 27547\n",
      "‚Üí Imbalance ratio: 2.63:1\n",
      "===============================================================\n",
      "\n",
      "===== Building Augmented Training Dataset =====\n",
      "Original counts ‚Üí Healthy=10465, Diseased=27547\n",
      "Healthy class replication multiplier: 2x\n",
      "Augmented training ‚Üí Healthy‚âà31395, Diseased‚âà27547\n",
      "Final ratio ‚âà 1:1\n",
      "===============================================================\n",
      "\n",
      "===== Preparing TF Datasets (Batched + Prefetched) =====\n",
      "‚úì Batch size    : 32\n",
      "‚úì Image size    : (224, 224)\n",
      "‚úì train/val/test datasets ready\n",
      "=========================================================\n",
      "\n",
      "‚Üí Extracting raw TF dataset for PyTorch pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 15:36:07.785005: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:8: Filling up shuffle buffer (this may take a while): 45558 of 54303\n",
      "2025-11-27 15:36:10.574776: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "2025-11-27 15:38:38.380581: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "df, histories = run_all_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca5606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: results/accuracy_by_model.png\n",
      "Saved: results/f1_by_model.png\n",
      "Saved: results/auc_by_model.png\n",
      "Saved: results/train_time_by_model.png\n",
      "Saved: results/ram_usage_by_model.png\n",
      "Saved: results/gpu_peak_by_model.png\n",
      "All training, evaluation, and comparison complete.\n",
      "Check models/ for checkpoints, results/ for CSV & plots, logs/ for TensorBoard.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 9 ‚Äî Visualization: Model Comparison Plots\n",
    "# ============================================================\n",
    "\n",
    "def plot_bar_metric(df, metric, ylabel, title, filename, sort=True):\n",
    "    \"\"\"\n",
    "    Creates a bar chart for any metric.\n",
    "    Automatically skips missing or empty metrics.\n",
    "    \"\"\"\n",
    "    if metric not in df.columns:\n",
    "        print(f\"‚ö† Metric '{metric}' not found. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    if df[metric].dropna().empty:\n",
    "        print(f\"‚ö† Metric '{metric}' has no values. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # Sort models for readability\n",
    "    if sort:\n",
    "        df = df.sort_values(metric, ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(df))\n",
    "    plt.bar(x, df[metric], color=\"#4F81BD\")\n",
    "\n",
    "    plt.xticks(x, df[\"model\"], rotation=35, ha=\"right\")\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(title, fontsize=15, fontweight=\"bold\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(\"results\", filename)\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"‚úì Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def generate_all_plots(results_df):\n",
    "    print(\"\\n===== Generating Comparison Plots =====\")\n",
    "\n",
    "    df_sorted = results_df.sort_values(\"test_accuracy\", ascending=False)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # CORE PERFORMANCE METRICS\n",
    "    # ------------------------------------------------------------\n",
    "    plot_bar_metric(\n",
    "        df_sorted,\n",
    "        metric=\"test_accuracy\",\n",
    "        ylabel=\"Accuracy\",\n",
    "        title=\"Model Comparison ‚Äî Test Accuracy\",\n",
    "        filename=\"accuracy_by_model.png\",\n",
    "    )\n",
    "\n",
    "    plot_bar_metric(\n",
    "        df_sorted,\n",
    "        metric=\"f1\",\n",
    "        ylabel=\"F1 Score\",\n",
    "        title=\"Model Comparison ‚Äî F1 Score\",\n",
    "        filename=\"f1_by_model.png\",\n",
    "    )\n",
    "\n",
    "    plot_bar_metric(\n",
    "        df_sorted,\n",
    "        metric=\"auc\",\n",
    "        ylabel=\"AUC Score\",\n",
    "        title=\"Model Comparison ‚Äî ROC-AUC\",\n",
    "        filename=\"auc_by_model.png\",\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TRAINING TIME\n",
    "    # ------------------------------------------------------------\n",
    "    plot_bar_metric(\n",
    "        df_sorted,\n",
    "        metric=\"train_time_min\",\n",
    "        ylabel=\"Minutes\",\n",
    "        title=\"Training Time by Model (Minutes)\",\n",
    "        filename=\"training_time_by_model.png\",\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # MEMORY USAGE\n",
    "    # ------------------------------------------------------------\n",
    "    if \"mem_after_ram_gb\" in df_sorted.columns:\n",
    "        plot_bar_metric(\n",
    "            df_sorted,\n",
    "            metric=\"mem_after_ram_gb\",\n",
    "            ylabel=\"RAM (GB)\",\n",
    "            title=\"RAM Usage After Training\",\n",
    "            filename=\"ram_usage_after_training.png\",\n",
    "            sort=False,\n",
    "        )\n",
    "\n",
    "    if \"mem_after_gpu_peak_gb\" in df_sorted.columns:\n",
    "        plot_bar_metric(\n",
    "            df_sorted,\n",
    "            metric=\"mem_after_gpu_peak_gb\",\n",
    "            ylabel=\"Peak GPU Memory (GB)\",\n",
    "            title=\"GPU Peak Memory by Model\",\n",
    "            filename=\"gpu_peak_memory_by_model.png\",\n",
    "            sort=False,\n",
    "        )\n",
    "\n",
    "    print(\"\\n===== ALL PLOTS GENERATED SUCCESSFULLY =====\")\n",
    "    print(\"üìÅ Check 'results/' folder for all graphs.\")\n",
    "    print(\"üìÅ Confusion matrices are inside 'models/' directory.\")\n",
    "    print(\"üìÅ Comparison table saved at: results/model_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2297b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
